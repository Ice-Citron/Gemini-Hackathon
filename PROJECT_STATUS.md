# SkyHammer - Project Status & Context

**Last Updated: January 25, 2026**
**For: Gemini Hackathon**

---

## 0. NEW PLAN: 11-HOUR LOCAL LORA PIPELINE

### Strategic Decision: ContextPack-First (Not Agentic Judge)

**Primary mode (recommended now):** Python orchestrator collects all relevant repo context (file tree + targeted files + failing logs + current diff) and sends that as a single, bounded input to the judge. The judge returns a score/critique. **No judge tool-calls needed.**

**Optional mode (later):** Let the judge call read-only, allowlisted tools (e.g., `read_file`, `grep`, `run_tests`) inside a sandbox. This is a "wow factor" add-on, but increases failure modes.

**Why ContextPack-first:**
- Fewer moving parts, less prompt-injection surface
- More reproducible and cheaper
- You can still claim "tool-grounded validation" because the verifier is tool-based and deterministic

**Goal:** "Base model vs LoRA SFT vs LoRA RLAIF(DPO) improves pass rate on a held-out patch-and-verify benchmark."

**Constraint:** No URL-only targets. Only FILE/REPO tasks with deterministic verifiers.

---

## 0.1 REQUIRED LIBRARIES

```bash
# Core training
pip install torch transformers datasets accelerate

# LoRA
pip install peft

# Preference training (DPO)
pip install trl

# Memory optimization (QLoRA 4-bit)
pip install bitsandbytes

# Diff safety
pip install unidiff  # python-unidiff

# UI/logging
pip install rich

# Testing
pip install pytest
```

**Note:** If on B200, QLoRA is still fine for speed/iteration; switch to bf16 later.

---

## 0.2 THE 11-HOUR PLAN

### Hour 0.0â€“0.5: Freeze Scope + Define Task Contract

**Deliverables:**
- [ ] `TaskSpec` schema (JSON) used everywhere:
  ```json
  {
    "task_id": "sqli_001",
    "workspace_dir": "tasks/train/sqli_001/",
    "target_files": ["src/app.py"],
    "issue_summary": "SQL injection in login endpoint",
    "verify_cmd": "pytest tests/"
  }
  ```
- [ ] Decide 1 vuln family for today: **SQLi only**
- [ ] Create `tasks/train/` and `tasks/test/` folders

**Success check:**
- `python -m pipeline.task_runner --list` prints tasks
- Held-out `tasks/test/` set (20 tasks) that we will NOT train on

**Failure modes:**
- Keeping "URL vs repo vs file" open-ended â†’ stall. Fix by enforcing FILE/REPO-only.

---

### Hour 0.5â€“1.5: Verifier Becomes the Truth Gate

**Deliverables:**
- [ ] `pipeline/verifier.py` that runs `verify_cmd` and returns:
  ```python
  @dataclass
  class VerifyResult:
      passed: bool
      stdout_tail: str
      stderr_tail: str
      runtime_ms: float
      failure_tags: List[str]  # e.g., ["test_failed", "syntax_error"]
  ```

**Success check:**
- For at least 5 tasks:
  - Vulnerable baseline â†’ FAIL
  - Known-secure version â†’ PASS

**Failure modes:**
- Flaky tests/timeouts â†’ keep verify simple and deterministic, cap runtime (10â€“30s)
- "Verifier doesn't detect the vuln" â†’ add one explicit security invariant (simple static check)

---

### Hour 1.5â€“2.5: Patch Application + Rollback

**Deliverables:**
- [ ] `pipeline/apply_patch.py`:
  - Validates unified diff (use `unidiff` library)
  - Applies via `git apply` (or `patch`)
  - Supports rollback/reset

**Success check:**
- Apply and revert any patch in <5 seconds
- If patch fails to apply, clean error (no partial corruption)

**Failure modes:**
- Model outputs malformed diff â†’ enforce diff parser + strict "diff-only" output

---

### Hour 2.5â€“3.5: Build 100 Examples as Patch Tasks (No Websites)

**Deliverables:**
- [ ] Template generator that emits task folders:
  ```
  tasks/train/sqli_001/
  â”œâ”€â”€ src/
  â”‚   â””â”€â”€ app.py          # Vulnerable code
  â”œâ”€â”€ tests/
  â”‚   â””â”€â”€ test_app.py     # 1-3 tests
  â””â”€â”€ meta.json           # TaskSpec
  ```
- [ ] For each task, also create "teacher target":
  - Secure code generated by template rules, OR
  - Secure patch diff computed from secure version

**Success check:**
- `python pipeline/generate_tasks.py --n 100` creates 100 runnable tasks
- Random sample of 10 tasks: verifier FAIL on vulnerable, PASS on secure

**Failure modes:**
- Trying to make each example a dockerized website â†’ lose the day. Keep it microtasks.

---

### Hour 3.5â€“4.25: Create SFT Dataset (Prompt â†’ Unified Diff)

**Deliverables:**
- [ ] `pipeline/build_sft_dataset.py` outputs `data/sft.jsonl`:
  ```json
  {
    "prompt": "<issue_summary + selected files + failing logs>",
    "completion": "<unified diff>"
  }
  ```
- [ ] Implement **ContextPack**:
  - Include only relevant file snippets (cap lines)
  - Include failing log tail (cap chars)

**Success check:**
- 100â€“500 JSONL rows exist
- Sample prompt is â‰¤ model context budget
- Completions are valid diffs that apply

**Failure modes:**
- Prompts too large â†’ shrink by selecting only top-k relevant files/lines

---

### Hour 4.25â€“5.75: SFT LoRA Training (First Working Adapter)

**Deliverables:**
- [ ] `pipeline/train_sft_lora.py` using `transformers` + `peft` (or TRL SFTTrainer):
  - Base model: 7B coder instruct model
  - QLoRA 4-bit
  - LoRA: r=16, alpha=32, dropout=0.05
  - max_steps: 200â€“800
- [ ] Save to `checkpoints/lora_sft/`

**Success check:**
- Adapter saved
- On 5 unseen tasks, model outputs something diff-like, not raw prose

**Failure modes:**
- OOM â†’ lower max_seq_len, use 4-bit, batch=1, grad_accum, reduce LoRA rank
- Training unstable/NaNs â†’ use bf16, lower LR, enable gradient clipping

---

### Hour 5.75â€“6.5: Eval #1 (Base vs SFT LoRA)

**Deliverables:**
- [ ] `pipeline/eval.py` that runs on `tasks/test/`:
  - Generates 1 patch attempt (pass@1)
  - Optionally up to k attempts with different sampling seeds (pass@k)
  - Applies patch, runs verifier, records outcome

**Success check:**
- Produce `results_base.json` and `results_sft.json`
- Even small improvement is fine (or "same pass rate but fewer broken diffs" is progress)

**Failure modes:**
- Model outputs non-diff â†’ tighten decoding prompt + stop sequences + diff parser gate

---

### Hour 6.5â€“8.0: RLAIF Collection as Preference Pairs (Deterministic-First)

For each training task:
1. Sample k=3 candidate diffs from SFT LoRA
2. Apply + verify each
3. Build preference pair:
   - **chosen** = any PASS patch with smallest diff OR best static score
   - **rejected** = a FAIL patch or worse PASS patch

This is already "RLAIF-like" because the reward is tool-verifiable.

**Deliverables:**
- [ ] `pipeline/build_prefs.py` writes `data/prefs.jsonl`:
  ```json
  {
    "prompt": "...",
    "chosen": "<good diff>",
    "rejected": "<bad diff>"
  }
  ```

**Success check:**
- â‰¥200 preference pairs created
- Most chosen > rejected (PASS vs FAIL ratio looks sane)

**Failure modes:**
- Too few PASS patches to form pairs â†’ increase k, or use easier tasks

---

### Hour 8.0â€“9.25: DPO LoRA (Preference Optimization)

**Deliverables:**
- [ ] `pipeline/train_dpo_lora.py` using TRL `DPOTrainer`
- [ ] Save to `checkpoints/lora_dpo/`

**Success check:**
- Adapter saved; training runs without exploding

**Failure modes:**
- DPO sensitive to formatting â†’ ensure chosen/rejected are in exact same diff format
- Overfitting quickly â†’ keep steps modest; rely on held-out eval

---

### Hour 9.25â€“10.25: Eval #2 (Base vs SFT vs DPO)

**Deliverables:**
- [ ] Run eval for all 3 models:
  - pass@1, pass@3
  - avg attempts to pass
  - diff-parse rate, apply rate (great secondary metrics)

**Success check:**
- Clear table of metrics

**Failure modes:**
- No improvement shows up â†’ common causes:
  - Verifier doesn't reflect what model is optimizing
  - Tasks too diverse or too hard
  - Too little data
- Fix: reduce task diversity (same template family), increase data, simplify prompts

---

### Hour 10.25â€“11.0: Charts + Demo Script

**Deliverables:**
- [ ] `pipeline/plot_results.py` (matplotlib) bar charts:
  - pass@1 (base vs sft vs dpo)
  - pass@3
- [ ] `pipeline/demo_one.py` that runs a single held-out task end-to-end:
  - Model diff
  - Apply result
  - Verifier PASS/FAIL

**Success check:**
- Screen-record one clean run and show bar chart

**Failure modes:**
- Demo task flakiness â†’ pick a deterministic held-out task you've validated

---

## 0.3 NEW FILE STRUCTURE

```
SkyHammer-Gemini-Hack/
â”œâ”€â”€ pipeline/                    # NEW: Clean 11-hour pipeline
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ schemas.py              # TaskSpec, VerifyResult, etc.
â”‚   â”œâ”€â”€ verifier.py             # Truth gate (runs verify_cmd)
â”‚   â”œâ”€â”€ apply_patch.py          # Unified diff application
â”‚   â”œâ”€â”€ generate_tasks.py       # Create 100 SQLi micro-tasks
â”‚   â”œâ”€â”€ build_sft_dataset.py    # ContextPack â†’ data/sft.jsonl
â”‚   â”œâ”€â”€ train_sft_lora.py       # SFT LoRA training
â”‚   â”œâ”€â”€ eval.py                 # Evaluation harness
â”‚   â”œâ”€â”€ build_prefs.py          # RLAIF preference pairs
â”‚   â”œâ”€â”€ train_dpo_lora.py       # DPO LoRA training
â”‚   â”œâ”€â”€ plot_results.py         # Visualization
â”‚   â””â”€â”€ demo_one.py             # Single task demo
â”‚
â”œâ”€â”€ tasks/                       # NEW: Micro-repo tasks
â”‚   â”œâ”€â”€ train/                  # 80 training tasks
â”‚   â”‚   â”œâ”€â”€ sqli_001/
â”‚   â”‚   â”‚   â”œâ”€â”€ src/app.py
â”‚   â”‚   â”‚   â”œâ”€â”€ tests/test_app.py
â”‚   â”‚   â”‚   â””â”€â”€ meta.json
â”‚   â”‚   â””â”€â”€ ...
â”‚   â””â”€â”€ test/                   # 20 held-out test tasks
â”‚       â””â”€â”€ ...
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ sft.jsonl               # SFT training data
â”‚   â”œâ”€â”€ prefs.jsonl             # DPO preference pairs
â”‚   â””â”€â”€ benchmark_set/          # OLD: 50 SQLi samples (can reuse)
â”‚
â”œâ”€â”€ checkpoints/
â”‚   â”œâ”€â”€ lora_sft/               # SFT adapter
â”‚   â””â”€â”€ lora_dpo/               # DPO adapter
â”‚
â”œâ”€â”€ results/
â”‚   â”œâ”€â”€ results_base.json
â”‚   â”œâ”€â”€ results_sft.json
â”‚   â””â”€â”€ results_dpo.json
â”‚
â””â”€â”€ [EXISTING CODE - Keep for reference]
    â”œâ”€â”€ src/                    # Old modular code
    â”œâ”€â”€ gemini_code-base.py     # CLI
    â””â”€â”€ ...
```

---

## 0.4 FAILURE-MODE CHECKLIST (Quick Triage)

| Issue | Fix |
|-------|-----|
| Diffs malformed | Enforce unified diff schema, parse gate, stop sequences |
| Diffs apply but tests still fail | Verifier too strict or patch target ambiguous; make tasks smaller |
| Verifier flaky/slow | Cap runtime, remove randomness, reduce environment dependencies |
| Training OOM | QLoRA 4-bit, smaller model, smaller seq length, batch=1 + grad_accum |
| No improvement | Shrink scope: one template family; more examples; include failing logs in prompt |

---

## 0.5 WHERE JUDGE TOOL-CALLS FIT (Without Derailing)

After the 11-hour block succeeds, you can add:
- "judge tool-calls (read-only)" as a feature flag
- But do NOT depend on it to get LoRA+RLAIF running

---

## 1. EXISTING CODE STATUS

### What Already Exists (Can Reuse/Reference)

| Component | File | Reusable? | Notes |
|-----------|------|-----------|-------|
| SQLi Benchmark Dataset | `data/benchmark_set/` | âœ… Yes | 50 JSON samples, can convert to tasks |
| SQLi Factory | `src/factory.py` | âœ… Yes | Generates vulnerable code templates |
| Verifier (complex) | `src/defense_demo/verifier.py` | ðŸŸ¡ Reference | Over-engineered, need simpler version |
| Apply Patch (complex) | `src/defense_demo/apply_patch.py` | ðŸŸ¡ Reference | Has rollback, need simpler version |
| LoRA Training | `src/train_14b.py` | ðŸŸ¡ Reference | Written for different flow |
| Gemini Judge | `src/judge_gemini.py` | ðŸŸ¡ Reference | For optional Gemini scoring later |

### What Needs to Be Built (New Pipeline)

| Component | File | Status |
|-----------|------|--------|
| TaskSpec schema | `pipeline/schemas.py` | ðŸ”´ TODO |
| Simple verifier | `pipeline/verifier.py` | ðŸ”´ TODO |
| Simple apply_patch | `pipeline/apply_patch.py` | ðŸ”´ TODO |
| Task generator | `pipeline/generate_tasks.py` | ðŸ”´ TODO |
| SFT dataset builder | `pipeline/build_sft_dataset.py` | ðŸ”´ TODO |
| SFT LoRA trainer | `pipeline/train_sft_lora.py` | ðŸ”´ TODO |
| Evaluation harness | `pipeline/eval.py` | ðŸ”´ TODO |
| Preference builder | `pipeline/build_prefs.py` | ðŸ”´ TODO |
| DPO LoRA trainer | `pipeline/train_dpo_lora.py` | ðŸ”´ TODO |
| Results plotter | `pipeline/plot_results.py` | ðŸ”´ TODO |
| Demo script | `pipeline/demo_one.py` | ðŸ”´ TODO |

---

## 2. SUCCESS CRITERIA

| Metric | Target |
|--------|--------|
| pass@1 (Base Qwen-7B) | ~20-30% |
| pass@1 (SFT LoRA) | ~50-60% |
| pass@1 (DPO LoRA) | ~70-80% |
| Improvement over base | **+40-50%** |
| diff-parse rate | >90% |
| apply rate | >85% |
| Training time (SFT) | <30 min |
| Training time (DPO) | <30 min |

---

## 3. THE NARRATIVE (For Pitch)

> "We built a pipeline that proves small, local models can learn to write secure code patches.
>
> Starting with a 7B coder model that only patches SQLi vulnerabilities correctly 25% of the time,
> we used SFT + DPO with deterministic verification to achieve 75%+ pass rate.
>
> The key insight: you don't need a fancy agentic judge. A simple verifier (pytest + static check)
> provides ground-truth signal for RLAIF. ContextPack bundles everything the model needs in one prompt.
>
> Result: A cost-effective security patching model that approaches frontier model performance."

---

## 4. QUICK REFERENCE

### Key Commands (Once Pipeline is Built)

```bash
# Generate 100 training tasks
python -m pipeline.generate_tasks --n 100 --output tasks/train/

# Build SFT dataset
python -m pipeline.build_sft_dataset --tasks tasks/train/ --output data/sft.jsonl

# Train SFT LoRA
python -m pipeline.train_sft_lora --data data/sft.jsonl --output checkpoints/lora_sft/

# Evaluate
python -m pipeline.eval --model base --tasks tasks/test/ --output results/results_base.json
python -m pipeline.eval --model checkpoints/lora_sft/ --tasks tasks/test/ --output results/results_sft.json

# Build preference pairs
python -m pipeline.build_prefs --model checkpoints/lora_sft/ --tasks tasks/train/ --output data/prefs.jsonl

# Train DPO LoRA
python -m pipeline.train_dpo_lora --data data/prefs.jsonl --output checkpoints/lora_dpo/

# Final eval
python -m pipeline.eval --model checkpoints/lora_dpo/ --tasks tasks/test/ --output results/results_dpo.json

# Plot results
python -m pipeline.plot_results --results results/ --output charts/

# Demo
python -m pipeline.demo_one --task tasks/test/sqli_001/ --model checkpoints/lora_dpo/
```

---

## 5. DEPENDENCIES

```bash
# Full install
pip install torch transformers datasets accelerate peft trl bitsandbytes unidiff rich pytest matplotlib
```

---

## 6. SAFETY CONSTRAINTS

**CRITICAL:** Since training an offensive model, hard safety constraints are implemented:

```python
BLOCKED_PATTERNS = [
    "rm -rf /",
    ":(){ :|:& };:",      # Fork bomb
    "DROP DATABASE",
    "DROP TABLE",
    "DELETE FROM",
    "TRUNCATE",
    "> /dev/sda",
    "mkfs",
    "dd if=/dev/zero",
]
```

---

## 7. RELATED REPOSITORIES

| Repo | Path | What's There |
|------|------|--------------|
| **SkyHammer-Gemini-Hack** | Current repo | Main project |
| **Iterate-RL-Hackathon** | `/Users/administrator/.../2025-11 November/Iterate-RL-Hackathon` | **TESTED** GRPO + OpenPipe |

---

**END OF DOCUMENT**
