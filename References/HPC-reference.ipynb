{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9df71887-c3a9-483f-9aa4-c2d61accfb63",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd ./Gemini-Hackathon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "87bef62e-dedf-411c-9bb7-b9127fa7f2a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "619bb004-a2c6-4df8-8567-c3e792a4b20d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installation of Python-3.11\n",
    "conda install -n base -c conda-forge python=3.11 -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ad44e82-81e9-495a-ae3f-02002b646537",
   "metadata": {},
   "outputs": [],
   "source": [
    "conda create -n skyhammer311 -c conda-forge python=3.11 -y\n",
    "conda activate skyhammer311\n",
    "which python\n",
    "\n",
    "python -m pip install -U pip\n",
    "pip install -r requirements.txt\n",
    "\n",
    "\n",
    "huggingface-cli login\n",
    "wandb login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a72796e-ff3f-4561-ae6f-f07d3d3d91d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Create Environment\n",
    "conda create -n skyhammer_serve -c conda-forge python=3.11 -y\n",
    "\n",
    "# 2. Activate\n",
    "conda activate skyhammer_serve\n",
    "which python\n",
    "\n",
    "# 3. Install vLLM (Clean install)\n",
    "pip install -r requirements-serve.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e449dc73-b704-4e31-aa12-28131d959bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just press Enter for defaults, mostly. \n",
    "# Key choices: \n",
    "# - Multi-GPU: YES\n",
    "# - Num machines: 1\n",
    "# - Num processes: 8\n",
    "# - Mixed precision: bf16 (Since you have L40s)\n",
    "accelerate config\n",
    "\n",
    "export PYTORCH_ALLOC_CONF=expandable_segments:True\n",
    "accelerate launch --multi_gpu --num_processes=8 src/train/train_ddp.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5816b353-115f-47b1-a9e6-5c78b996b1aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Set Memory Management Flag     # 2. Run on Single GPU (Safe Mode)\n",
    "export PYTORCH_ALLOC_CONF=expandable_segments:True\n",
    "CUDA_VISIBLE_DEVICES=0 python src/train/train_ddp.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fac1f419-d03f-4a98-9ed5-7d5fb0e20f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "conda deactivate\n",
    "conda remove --name skyhammer311 --all -y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd889f31-98c1-4f73-9b27-edc605e7e4ad",
   "metadata": {},
   "source": [
    "# VLLM SERVE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "984bbf14-29bc-4237-b744-e1019869673a",
   "metadata": {},
   "outputs": [],
   "source": [
    "vllm serve Qwen/Qwen2.5-Coder-32B-Instruct-AWQ \\\n",
    "  --tensor-parallel-size 2 \\\n",
    "  --enable-lora \\\n",
    "  --lora-modules skyhammer=shng2025/SkyHammer-32B-v1 \\\n",
    "  --port 8000 \\\n",
    "  --api-key skyhammer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6af484b1-ab83-4d34-a3d6-86d495890641",
   "metadata": {},
   "outputs": [],
   "source": [
    "vllm serve Qwen/Qwen3-VL-32B-Instruct \\\n",
    "  --tensor-parallel-size 2 \\\n",
    "  --max-model-len 32768 \\\n",
    "  --port 8000 \\\n",
    "  --trust-remote-code \\\n",
    "  --api-key skyhammer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ddd6f64-9c03-4e70-a970-72886a048972",
   "metadata": {},
   "outputs": [],
   "source": [
    "huggingface-cli download unsloth/GLM-4.7-GGUF \\\n",
    "  --include \"Q4_K_M/*\" \\\n",
    "  --local-dir models/ \\\n",
    "  --local-dir-use-symlinks False\n",
    "\n",
    "\n",
    "# pip install \"llama-cpp-python[server]\"\n",
    "CMAKE_ARGS=\"-DGGML_CUDA=on\" pip install --upgrade --force-reinstall --no-cache-dir llama-cpp-python[server]\n",
    "\n",
    "python -m llama_cpp.server \\\n",
    "  --model models/Q4_K_M/GLM-4.7-Q4_K_M-00001-of-00005.gguf \\\n",
    "  --n_gpu_layers -1 \\\n",
    "  --n_ctx 32768 \\\n",
    "  --port 8000 \\\n",
    "  --api_key skyhammer\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58825cb2-2b55-4e21-92ea-a883eab28a7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run inside 'skyhammer311' environment\n",
    "accelerate launch --multi_gpu --num_processes=8 src/train/train_ddp-glm.py \\\n",
    "  --max_steps 10 \\\n",
    "  --logging_steps 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
